{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfMuITSUiJYE"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2XJcMWVd8A5gZbLT1Mbd8f5q8id_2QJzWvWHVpCEWDDA8XUmD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzUWv3HupQJl",
        "outputId": "bda505c2-64a9-49b7-e474-3b4a26dd9e51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok"
      ],
      "metadata": {
        "id": "zMvRiHwHl_PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install PyPDF2\n",
        "!pip install langchain sentence_transformers\n",
        "!pip install huggingface_hub\n",
        "!pip install openai"
      ],
      "metadata": {
        "id": "ZBkwBhrKwmG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile htmlTemplates.py\n",
        "css = '''\n",
        "<style>\n",
        ".chat-message {\n",
        "    padding: 0.5rem; border-radius: 0.25rem; margin-bottom: 0.5rem; display: flex\n",
        "}\n",
        ".chat-message.user {\n",
        "    background-color: #2b313e\n",
        "}\n",
        ".chat-message.bot {\n",
        "    background-color: #475063\n",
        "}\n",
        ".chat-message .avatar {\n",
        "  width: 5%;\n",
        "}\n",
        ".chat-message .avatar img {\n",
        "  max-width: 0.1px;\n",
        "  max-height: 0.1px;\n",
        "  border-radius: 5%;\n",
        "  object-fit: cover;\n",
        "}\n",
        ".chat-message .message {\n",
        "  width: 20%;\n",
        "  padding: 0 0.5rem;\n",
        "  color: #fff;\n",
        "}\n",
        "'''\n",
        "\n",
        "bot_template = '''\n",
        "<div class=\"chat-message bot\">\n",
        "    <div class=\"avatar\">\n",
        "        <img src=\"https://www.freeiconspng.com/img/3083\">\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "user_template = '''\n",
        "<div class=\"chat-message user\">\n",
        "    <div class=\"avatar\">\n",
        "        <img src=\"https://icon-library.com/icon/human-icon-png-1.html.html\">\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fisSf3f4zBFW",
        "outputId": "c16ae699-4900-4331-822e-3b70a9d10456"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting htmlTemplates.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import langchain\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import os\n",
        "from langchain.llms import HuggingFaceHub, OpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from htmlTemplates import css, bot_template, user_template\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "\n",
        "\n",
        "def get_raw_text(pdf_docs):\n",
        "    text = ''\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf)\n",
        "        for pages in pdf_reader.pages:\n",
        "            text += pages.extract_text()\n",
        "    st.write(\"Raw text extracted\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splliter = CharacterTextSplitter(\n",
        "        separator='\\n',\n",
        "        chunk_size=100,\n",
        "        chunk_overlap=20,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splliter.split_text(text)\n",
        "    st.write(\"Chunks splited\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorestore(text_chunks):\n",
        "    embeddings = HuggingFaceEmbeddings()\n",
        "    vectorestore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    st.write(\"Stored to Vectorestore\")\n",
        "    return vectorestore\n",
        "\n",
        "\n",
        "def get_conversation(vectorstore):\n",
        "    llm=OpenAI()\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        memory=memory,\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "\n",
        "def handle_user_input(user_question):\n",
        "    response = st.session_state.conversation({'question':user_question})\n",
        "    st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i%2==0:\n",
        "            st.write(user_template.replace(\"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "\n",
        "        else:\n",
        "            st.write(bot_template.replace(\"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "st.set_page_config(page_title='Chat with your PDFs', page_icon=\":books:\")\n",
        "\n",
        "if 'conversation' not in st.session_state:\n",
        "    st.session_state.conversation=None\n",
        "\n",
        "\n",
        "st.header('Chat with your PDFs :books:')\n",
        "user_question = st.text_input('Enter your query here')\n",
        "\n",
        "if user_question:\n",
        "    handle_user_input(user_question)\n",
        "\n",
        "with st.sidebar:\n",
        "    st.subheader('Your Documents')\n",
        "    pdf_docs = st.file_uploader('Upload your files here and press \"Process\"', accept_multiple_files=True)\n",
        "    if st.button('Process'):\n",
        "        with st.spinner('Processing'):\n",
        "          raw_text = get_raw_text(pdf_docs)\n",
        "\n",
        "          #breaking the text into text chunks\n",
        "          text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "          #converting the chunks into embeddings and storing them in vector database\n",
        "          vectorstore = get_vectorestore(text_chunks)\n",
        "\n",
        "          #creating conversational chatbot\n",
        "          st.session_state.conversation = get_conversation(vectorstore)\n",
        "          st.write(\"Process Completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz2M8B4okA8G",
        "outputId": "5ef0af12-e2fd-40a7-dd28-60585ec792cc"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cloHJ0VhnCpK",
        "outputId": "4957fecf-5888-4d75-9e79-f03ab36dce44"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\thtmlTemplates.py  __pycache__  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "oDTHGtVHnEsb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjPFpeT8ttQ6",
        "outputId": "44efadb2-e1ff-4d85-e08e-821f264deacd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUJdeYlAniFS",
        "outputId": "303180e8-6bb5-4cd5-d39c-f1295df45108"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-10-26T20:45:08+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://eb95-34-91-44-131.ngrok-free.app\" -> \"http://localhost:8501\">"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}